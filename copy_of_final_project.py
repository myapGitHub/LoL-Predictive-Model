# -*- coding: utf-8 -*-
"""Copy of Final Project

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dyIq_yuaHuqknR0eNptOrJlPJ6yfVuQv

# CS513 - Final Project

Name: Alex Ha, Kanta Nakano, Matthew Yap

CWID: 20006399

"I pledge my honor that I have abided by the Stevens Honor System."

# Set-up for Final Project
"""

from google.colab import drive
drive.mount('/content/drive')
#Mounting so anyone can access csv's

!pip install minisom
!pip install ydata-profiling

"""### Importing the required libraries"""

import numpy as np
import pandas as pd
import scipy.stats as stats
import pylab
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import StandardScaler
from ydata_profiling import ProfileReport
from sklearn.cluster import KMeans

from sklearn.neighbors import KNeighborsClassifier

from sklearn.naive_bayes import CategoricalNB
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline

from sklearn.tree import DecisionTreeClassifier

from sklearn.cluster import AgglomerativeClustering

from sklearn.svm import SVC

from sklearn.neural_network import MLPClassifier
from sklearn import metrics

from minisom import MiniSom
import math

from sklearn.metrics import accuracy_score
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

"""### Importing the required dataset"""

df = pd.read_csv('/content/drive/My Drive/CS513 Final Project/Challenger_Ranked_Games_10minute.csv')

print(df.describe())
df.head(10)
df.tail(10)

print(df.describe())
df.head(10)
df.tail(10)

df.info()

"""### Data Cleaning / Exploratory Data Analysis / Data Normalization / Dataset Train - Test Split"""

# Shows NaN values in data frame if it exists
print(df[df.isna().any(axis=1)])

# Drop NaN values
df = df.dropna()
df = df.select_dtypes(exclude=['object'])

# Shows NaN values in data frame if it still exists
print(df[df.isna().any(axis=1)])

# Explanatory Data Analysis
summary = df.describe()
print(summary)
df.head(10)
df.tail(10)

df.info()

subset = df[['blueTotalGolds', 'redTotalGolds']]  # Replace with relevant columns

# Create the box plot
subset.plot(kind="box", vert=True, figsize=(10, 6))
plt.title("Box Plot of Game Statistics", fontsize=12)
plt.xlabel("Features")
plt.ylabel("Value")
plt.show()

subset = df[['blueKill', 'redKill']]  # Replace with relevant columns

# Create the box plot
subset.plot(kind="box", vert=True, figsize=(10, 6))
plt.title("Box Plot of Game Statistics", fontsize=12)
plt.xlabel("Features")
plt.ylabel("Value")
plt.show()

subset = df[['blueAvgLevel', 'redAvgLevel']]  # Replace with relevant columns

# Create the box plot
subset.plot(kind="box", vert=True, figsize=(10, 6))
plt.title("Box Plot of Game Statistics", fontsize=12)
plt.xlabel("Features")
plt.ylabel("Value")
plt.show()

subset = df[['blueTotalLevel', 'redTotalLevel']]  # Replace with relevant columns

# Create the box plot
subset.plot(kind="box", vert=True, figsize=(10, 6))
plt.title("Box Plot of Game Statistics", fontsize=12)
plt.xlabel("Features")
plt.ylabel("Value")
plt.show()

profile = ProfileReport(df, title = "Data Analysis")
profile

#Dataless attributes
df = df.drop('blueFirstBlood', axis = 1)
df = df.drop('redFirstBlood', axis = 1)

#redWins drop
df = df.drop('redWins', axis = 1)

#dropping duplicate rows

df = df.drop_duplicates()

# Splitting the dataset
attr = df.drop('blueWins', axis=1)  # features
target = df['blueWins'] # target variable

## using standard scaler instead of min max scaler
scaler = StandardScaler()

# Fit and transform the data
attr = pd.DataFrame(scaler.fit_transform(attr), columns=attr.columns)

"""# KNN Methodology"""

# Dataset training and testing split
attr_train, attr_test, target_train, target_test = train_test_split(attr, target, test_size = 0.30, random_state = 3, shuffle = True)

# Resetting index due to NaN values
attr_train = attr_train.reset_index(drop=True)
attr_test = attr_test.reset_index(drop=True)
target_train = target_train.reset_index(drop=True)
target_test = target_test.reset_index(drop=True)

# Implement of algorithm
k_values = [3,5,10, 20]

for k in k_values:
    knn = KNeighborsClassifier(n_neighbors = k, weights='distance')
    knn.fit(attr_train, target_train)
    target_pred = knn.predict(attr_test)
    accuracy = round(np.mean(target_test==target_pred) * 100, 2)
    print(f'Accuracy of model with k = {k}: {accuracy}%')
    print('')

# Confusion Matrix Implementation
cm=confusion_matrix(target_test, target_pred)
print('Confusion Matrix')
print(confusion_matrix(target_test, target_pred))
print()

# Accuracy Rate Implementation
print('Accuracy score')
print(accuracy_score(target_test, target_pred))
print()

# Classification Reports Implementation
print('Classification Report')
print(classification_report(target_test, target_pred))

# Check Predictions
attr_test['target_pred'] = target_pred
attr_test['target_actual'] = target_test
attr_test.head()

# Plot of Confusion Matrix
ax= plt.subplot()
sns.heatmap(cm, annot=True, fmt='g', ax=ax);  #annot=True to annotate cells, ftm='g' to disable scientific notation

# labels, title and ticks
ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels');
ax.set_title('Confusion Matrix');
ax.xaxis.set_ticklabels(['2','4'])
ax.yaxis.set_ticklabels(['2','4'])

"""#Random Forest Methodology"""

# Dataset training and testing split
attr_train, attr_test, target_train, target_test = train_test_split(attr, target, test_size = 0.30, random_state = 5, shuffle = True)

# resetting index due to NaN values
attr_train = attr_train.reset_index(drop=True)
attr_test = attr_test.reset_index(drop=True)
target_train = target_train.reset_index(drop=True)
target_test = target_test.reset_index(drop=True)

model = DecisionTreeClassifier(criterion='entropy', max_depth=30,max_leaf_nodes=25)

model.fit(attr_train,target_train)
target_pred = model.predict(attr_test)

from sklearn.metrics import confusion_matrix, classification_report, accuracy_score
print(f"\n Accuracy: {accuracy_score(target_test,target_pred)}  ")
print(f"\n Confusion Matrix:")
print(confusion_matrix(target_test,target_pred))
print(f"\n Classification Report:")
print(classification_report(target_test,target_pred))

ax= plt.subplot()
sns.heatmap(confusion_matrix(target_test,target_pred), annot=True, fmt='g', ax=ax);

ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels');
ax.set_title('Confusion Matrix');
ax.xaxis.set_ticklabels(['2','4'])
ax.yaxis.set_ticklabels(['2','4'])

"""#CART Methodology"""

# Dataset training and testing split
attr_train, attr_test, target_train, target_test = train_test_split(attr, target, test_size = 0.30, random_state = 5, shuffle = True)

# resetting index due to NaN values
attr_train = attr_train.reset_index(drop=True)
attr_test = attr_test.reset_index(drop=True)
target_train = target_train.reset_index(drop=True)
target_test = target_test.reset_index(drop=True)

#Create decision tree classifier
model = DecisionTreeClassifier()

#fit the model to classification tree and predict
model.fit(attr_train,target_train)
target_pred = model.predict(attr_test)

from sklearn.metrics import confusion_matrix, classification_report, accuracy_score
print(f"\n Accuracy: {accuracy_score(target_test,target_pred)}  ")
print(f"\n Confusion Matrix:")
print(confusion_matrix(target_test,target_pred))
print(f"\n Classification Report:")
print(classification_report(target_test,target_pred))

ax= plt.subplot()
sns.heatmap(confusion_matrix(target_test,target_pred), annot=True, fmt='g', ax=ax);

ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels');
ax.set_title('Confusion Matrix');
ax.xaxis.set_ticklabels(['2','4'])
ax.yaxis.set_ticklabels(['2','4'])

"""# SVM Methodology"""

# Split the data into training and testing sets
attr_train, attr_test, target_train, target_test = train_test_split(attr, target, test_size=0.25, random_state=7, shuffle=True)

# Resetting index due to NaN values
attr_train = attr_train.reset_index(drop=True)
attr_test = attr_test.reset_index(drop=True)
target_train = target_train.reset_index(drop=True)
target_test = target_test.reset_index(drop=True)

# instantiate classifier
svc=SVC()

# fit classifier to training set
svc.fit(attr_train,target_train)

# make predictions on test set
target_pred=svc.predict(attr_test)

# Confusion Matrix Implementation
cm=confusion_matrix(target_test, target_pred)
print('Confusion Matrix')
print(confusion_matrix(target_test, target_pred))
print()

# Accuracy Rate Implementation
print('Accuracy score')
print(accuracy_score(target_test, target_pred))
print()

# Classification Reports Implementation
print('Classification Report')
print(classification_report(target_test, target_pred))

attr_test['target_pred'] = target_pred
attr_test['target_actual'] = target_test
attr_test.head()

# Plot of Confusion Matrix
ax= plt.subplot()
sns.heatmap(cm, annot=True, fmt='g', ax=ax);  #annot=True to annotate cells, ftm='g' to disable scientific notation

# labels, title and ticks
ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels');
ax.set_title('Confusion Matrix');
ax.xaxis.set_ticklabels(['2','4'])
ax.yaxis.set_ticklabels(['2','4'])

"""# ANN Methodology"""

# Split the data into training and testing sets
attr_train, attr_test, target_train, target_test = train_test_split(attr, target, test_size=0.25, random_state=7, shuffle=True)

# Resetting index due to NaN values
attr_train = attr_train.reset_index(drop=True)
attr_test = attr_test.reset_index(drop=True)
target_train = target_train.reset_index(drop=True)
target_test = target_test.reset_index(drop=True)

# Artificial Neural Network - # Five Nodes in Hidden Layer
model = MLPClassifier(hidden_layer_sizes=(5,), max_iter=10000)
model.fit(attr_train, target_train)
target_pred = model.predict(attr_test)

# Loss Curve Implementation
sns.lineplot(x=range(len(model.loss_curve_)), y=model.loss_curve_)
plt.title('Loss Curve')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.show()

# Confusion Matrix Implementation
cm=confusion_matrix(target_test, target_pred)
print('Confusion Matrix')
print(confusion_matrix(target_test, target_pred))
print()

# Accuracy Rate Implementation
print('Accuracy score')
print(accuracy_score(target_test, target_pred))
print()

# Classification Reports Implementation
print('Classification Report')
print(classification_report(target_test, target_pred))

attr_test['target_pred'] = target_pred
attr_test['target_actual'] = target_test
attr_test.head()

# Plot of Confusion Matrix
ax= plt.subplot()
sns.heatmap(cm, annot=True, fmt='g', ax=ax);  #annot=True to annotate cells, ftm='g' to disable scientific notation

# labels, title and ticks
ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels');
ax.set_title('Confusion Matrix');
ax.xaxis.set_ticklabels(['2','4'])
ax.yaxis.set_ticklabels(['2','4'])

"""# SOM Methodology"""

# Setting up for SOM

features = df.drop(columns =['blueWins'])
targets = df['blueWins']

sc = MinMaxScaler(feature_range = (0,1))
features = sc.fit_transform(features)

sigma = 1.5
lr = 0.5
n_features = features.shape[1]
n_samples = features.shape[0]
print(f"number of samples: {n_samples}")

map_size = 5 * math.sqrt(n_samples)
map_height = map_width = math.ceil(math.sqrt(map_size))

print(f'(map_height, map_width) = ({map_height}, {map_width})')
print(f'Number of features: {n_features}')

# SOM Methodology
som = MiniSom(x=map_height, y=map_width, input_len=n_features, sigma=sigma, learning_rate=lr,
              neighborhood_function='gaussian', random_seed=123)

som.pca_weights_init(features)
som.train(data=features, num_iteration=50000, verbose=True)  # random training

# Distance Map
print(f'Shape: {som.distance_map().shape}')
print(f'First Line: {som.distance_map().T}')

# Frequency
frequencies = som.activation_response(features)
print(f'Frequencies:\n {np.array(frequencies, np.uint)}')
print(frequencies)

groups = AgglomerativeClustering(n_clusters=2, metric='euclidean',linkage='average')

clusters=groups.fit_predict(features)

df_cluster=pd.DataFrame({'Actual':target,'Cluster':clusters})
# Create a cross-tabulation
cross_tab = pd.crosstab(df_cluster['Actual'], df_cluster['Cluster'])

print(cross_tab)

wcss = []
for k in range(1, 50):  # Test values of K from 1 to 10
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(features)
    wcss.append(kmeans.inertia_)

# Plot WCSS vs. K
plt.plot(range(1, 50), wcss, marker='o')
plt.title('Elbow Method')
plt.xlabel('Number of Clusters (K)')
plt.ylabel('WCSS')
plt.show()

num_clusters = 45
kmeans = KMeans(n_clusters=num_clusters, random_state=12)
kmeans.fit(features)
labels = kmeans.labels_
centers = kmeans.cluster_centers_

df_cluster=pd.DataFrame({'Actual':target,'Cluster':labels})
# Create a cross-tabulation
cross_tab = pd.crosstab(df_cluster['Actual'], df_cluster['Cluster'])

print(cross_tab)

plt.figure(figsize=(map_height, map_width))

# plot U-matrix
u_matrix = som.distance_map().T
plt.pcolor(u_matrix, cmap='bone_r')
plt.colorbar()

# plot markers
markers = ['^', 's']
colors = ['b', 'r']
w0=[]
w1=[]
lbl=[]
mkrs=[]

for feature, label in zip(features, target):
    encoded = int(target)

    w = som.winner(feature)
    w0=np.concatenate((w0, w[0]), axis=None)
    w1=np.concatenate((w1, w[1]), axis=None)
    lbl=np.concatenate((lbl, label), axis=None)
    mkrs=np.concatenate((mkrs,[label]), axis=None)

    plt.plot(w[0] + 0.5, w[1] + 0.5,
        markers[encoded], markeredgecolor = colors[encoded],
        markerfacecolor = 'None', markersize = 10, markeredgewidth = 1)

plt.show()